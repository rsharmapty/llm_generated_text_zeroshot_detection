# llm_generated_text_zeroshot_detection
This project provides a Python script to evaluate a zero-shot LLM-generated text detection method. The script uses a large language model (LLM) to perform detection and runs both white-box and black-box experiments to measure performance.

# Zero-Shot LLM Content Detector
This project provides a Python script to evaluate a zero-shot LLM-generated text detection method. The script uses a large language model (LLM) to perform detection and runs both white-box and black-box experiments to measure performance.

### Features

1. Zero-Shot Detection: Utilizes a specified LLM (by default, Llama 3) to detect machine-generated text without explicit training.
2. Dual Experiments: Conducts two main experiments:
3. White-box: Detects text generated by the same LLM used for detection.
4. Black-box: Detects text from various other LLMs from the MAGE dataset.
5. MAGE Dataset Integration: Loads and processes data from the yaful/MAGE dataset on Hugging Face.
6. Performance Metrics: Calculates and reports key metrics like Accuracy, Precision, Recall, F1-Score, and AUROC.
7. Data Visualization: Generates and saves plots for ROC curves, Precision-Recall curves, and dataset characteristics.
8. Output Management: Saves all detailed results and metrics to a timestamped Excel file for easy analysis.

### Prerequisites
Before running the script, ensure you have the following:
Python 3.8+
A Hugging Face Account: A valid account is required to access the models and datasets.

### Setup
1. Clone the Repository:
git clone https://github.com/rsharmapty/llm_generated_text_zeroshot_detection.git
cd your-repository


2. Install Dependencies:
It is recommended to use a virtual environment.
pip install torch pandas numpy matplotlib seaborn scikit-learn datasets transformers accelerate

(Note: accelerate is often required for large model loading.)
Hugging Face Token:
Models like meta-llama/Meta-Llama-3-8B-Instruct and datasets like yaful/MAGE are gated and require authentication. You must provide a Hugging Face token with read access.
Go to your Hugging Face settings and create a new token with "read" role.
Open the main.py file and replace "Your hugging face Token here" with your actual token:
HF_TOKEN = "hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"


### Configuration
You can customize the experiment by modifying the variables at the top of the main.py file:
1. LLAMA_MODEL_NAME: The model used for detection and white-box generation.
2. PERTURBATION_MODEL_NAME: The model used for text perturbations (defaults to a smaller, faster model).
3. N_PERTURBATIONS: The number of perturbations to perform for each text during detection.
4. NUM_SAMPLES_TO_TAKE_PER_CLASS: The number of human and other LLM samples to load from the MAGE dataset.
5. MAX_TEXT_LENGTH_WORDS: Filters the dataset to include only texts below this word count.

How to Run
Simply execute the main.py script from your terminal:
python main.py


The script will handle data loading, text generation, detection, and saving the results.

### Results
The script will create a new directory named results/ in the project root. This directory will contain:
1. Plots: PNG files of the ROC and Precision-Recall curves for each experiment.
2. Excel Output: A timestamped Excel file (zero_shot_detection_results_... .xlsx) containing two sheets:
3. Detailed Results: A table with ground truth, detection scores, and predictions for each text.
4. Metrics: A summary table of the key performance metrics for each experiment.

### Resources for Inferencing

Running this script requires significant computational resources, especially for a large model like Llama 3.
GPU is Highly Recommended: The script will automatically use a CUDA-enabled GPU if available. A GPU with at least 32 GB of VRAM is strongly recommended to load the Llama 3 8B model and its required dependencies without running into memory issues.
CPU: While the script can run on a CPU, the process will be extremely slow due to the size of the models and the number of perturbations performed.
